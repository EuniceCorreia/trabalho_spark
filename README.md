# Trabalho Apache Spark com Delta Lake e Apache Iceberg

## Descrição
Trabalho sobre o uso do Apache Spark para processamento de dados em larga escala, com foco em tabelas de data lake: **Delta Lake** (para transações ACID) e **Apache Iceberg** (para gerenciamento de metadados e particionamento avançado).

O projeto inclui:
- Scripts PySpark para criar e manipular tabelas Delta e Iceberg.
- Notebook Jupyter com análise de dados reais (leitura de Parquet, particionamento, time travel).
- Documentação gerada com MkDocs (exemplos de INSERT, UPDATE, DELETE, MERGE).
- Diagrama ER do cenário (usando Matplotlib/NetworkX).

## Participantes
- Eunice Correia
- Maria Laura
- Vitoria Viana

