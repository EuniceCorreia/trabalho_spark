{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39c2a67d-94d9-4cc0-8219-a1917fbb31ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/02 19:22:05 WARN Utils: Your hostname, DESKTOP-1P6TETU resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/10/02 19:22:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/nice_correia/.cache/pypoetry/virtualenvs/trabalho-spark-RjY8yXlH-py3.12/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/nice_correia/.ivy2/cache\n",
      "The jars for the packages stored in: /home/nice_correia/.ivy2/jars\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-2d514e93-c184-40f6-b973-1964669db6f0;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.5.2 in central\n",
      ":: resolution report :: resolve 226ms :: artifacts dl 7ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.5.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   1   |   0   |   0   |   0   ||   1   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-2d514e93-c184-40f6-b973-1964669db6f0\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 1 already retrieved (0kB/6ms)\n",
      "25/10/02 19:22:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/02 19:22:07 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/10/02 19:22:07 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/10/02 19:22:07 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "25/10/02 19:22:07 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "25/10/02 19:22:07 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "25/10/02 19:22:07 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Spark Iniciado com ICEBERG LOCAL (Sem Lock/AWS)! ===\n",
      "Vers√£o do Spark: 3.5.7\n",
      "Config io-impl ATUAL: org.apache.iceberg.hadoop.HadoopFileIO\n",
      "‚úÖ IO local confirmado (HadoopFileIO - sem S3/AWS)!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cat√°logos dispon√≠veis: ['spark_catalog'] (deve incluir 'iceberg_catalog')\n",
      "‚ö†Ô∏è 'iceberg_catalog' n√£o listado ainda - teste CREATE para ativar.\n",
      "Teste 1: DROP TABLE OK! (Cat√°logo acess√≠vel).\n",
      "Teste 2: CREATE TABLE OK! (Tabela Iceberg criada).\n",
      "Teste 3: DROP TABLE teste OK! (Iceberg funcional - pronto para ETL).\n",
      "Teste 4: Tabelas em iceberg_catalog.default: 0 (deve ser 0 ap√≥s drop).\n",
      "\n",
      "üéâ TODOS OS TESTES ICEBERG PASSARAM! Cat√°logo local pronto para DDL/INSERT/UPDATE.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Limpa warehouse Iceberg (evita warnings de metadata antigos)\n",
    "shutil.rmtree(\"/tmp/iceberg\", ignore_errors=True)\n",
    "os.makedirs(\"/tmp/iceberg\", exist_ok=True)\n",
    "\n",
    "# SparkSession OTIMIZADA para ICEBERG LOCAL (sem lock-impl para evitar erro de construtor)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark-IPS-Iceberg-Local-Fixed-NoLock\") \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.2\") \\\n",
    "    .config(\"spark.sql.extensions\", \n",
    "            \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg_catalog.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg_catalog.warehouse\", \"file:///tmp/iceberg\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg_catalog.io-impl\", \"org.apache.iceberg.hadoop.HadoopFileIO\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg_catalog.default-spec\", \"v2\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")  # WARN para ver downloads (mude para \"ERROR\" ap√≥s sucesso)\n",
    "\n",
    "print(\"=== Spark Iniciado com ICEBERG LOCAL (Sem Lock/AWS)! ===\")\n",
    "print(f\"Vers√£o do Spark: {spark.version}\")\n",
    "\n",
    "# Verifica√ß√£o CR√çTICA: Confirma io-impl (deve ser HadoopFileIO)\n",
    "try:\n",
    "    io_impl = spark.conf.get(\"spark.sql.catalog.iceberg_catalog.io-impl\")\n",
    "    print(f\"Config io-impl ATUAL: {io_impl}\")\n",
    "    if \"HadoopFileIO\" in io_impl:\n",
    "        print(\"‚úÖ IO local confirmado (HadoopFileIO - sem S3/AWS)!\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è IO-impl suspeito - verifique se √© HadoopFileIO.\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao ler io-impl: {e}\")\n",
    "\n",
    "# Verifica√ß√£o de Cat√°logos (deve incluir iceberg_catalog ap√≥s init)\n",
    "try:\n",
    "    catalogs = [c.name for c in spark.catalog.listCatalogs()]\n",
    "    print(f\"Cat√°logos dispon√≠veis: {catalogs} (deve incluir 'iceberg_catalog')\")\n",
    "    if \"iceberg_catalog\" in catalogs:\n",
    "        print(\"‚úÖ Cat√°logo Iceberg registrado com sucesso!\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è 'iceberg_catalog' n√£o listado ainda - teste CREATE para ativar.\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao listar cat√°logos: {e}\")\n",
    "\n",
    "# Testa o cat√°logo Iceberg (DROP, CREATE simples - sem lock-impl)\n",
    "try:\n",
    "    # Teste 1: DROP TABLE teste\n",
    "    spark.sql(\"DROP TABLE IF EXISTS iceberg_catalog.default.iceberg_ips\")\n",
    "    print(\"Teste 1: DROP TABLE OK! (Cat√°logo acess√≠vel).\")\n",
    "\n",
    "    # Teste 2: CREATE simples (Iceberg table)\n",
    "    spark.sql(\"CREATE TABLE IF NOT EXISTS iceberg_catalog.default.test_table (id INT) USING iceberg\")\n",
    "    print(\"Teste 2: CREATE TABLE OK! (Tabela Iceberg criada).\")\n",
    "\n",
    "    # Teste 3: DROP teste\n",
    "    spark.sql(\"DROP TABLE IF EXISTS iceberg_catalog.default.test_table\")\n",
    "    print(\"Teste 3: DROP TABLE teste OK! (Iceberg funcional - pronto para ETL).\")\n",
    "\n",
    "    # Teste 4: Confirma listagem de tabelas vazia\n",
    "    tables_df = spark.sql(\"SHOW TABLES IN iceberg_catalog.default\")\n",
    "    print(f\"Teste 4: Tabelas em iceberg_catalog.default: {tables_df.count()} (deve ser 0 ap√≥s drop).\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro em um dos testes Iceberg: {e}\")\n",
    "    print(\"DETALHES: Se mencionar 'LockManager', confirme remo√ß√£o da config. Reinicie kernel se persistir.\")\n",
    "    print(\"Dica: Rode manualmente: spark.sql('SHOW CATALOGS').show() para debug.\")\n",
    "else:\n",
    "    print(\"\\nüéâ TODOS OS TESTES ICEBERG PASSARAM! Cat√°logo local pronto para DDL/INSERT/UPDATE.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb2103da-d13c-4a0b-a363-207965abc1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diret√≥rio do notebook: /home/nice_correia/trabalho_spark/notebooks\n",
      "Root do projeto detectado: /home/nice_correia/trabalho_spark\n",
      "Caminho absoluto do CSV: /home/nice_correia/trabalho_spark/data/ips_brasil.csv\n",
      "Arquivo existe? True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dados IPS full carregados de '/home/nice_correia/trabalho_spark/data/ips_brasil.csv': 5570 linhas, 79 colunas.\n",
      "\n",
      "Schema das colunas chave:\n",
      "root\n",
      " |-- C√≥digo IBGE: integer (nullable = true)\n",
      " |-- Munic√≠pio: string (nullable = true)\n",
      " |-- UF: string (nullable = true)\n",
      " |-- √çndice de Progresso Social: double (nullable = true)\n",
      "\n",
      "+-----------+--------------------------+---+--------------------------+\n",
      "|C√≥digo IBGE|Munic√≠pio                 |UF |√çndice de Progresso Social|\n",
      "+-----------+--------------------------+---+--------------------------+\n",
      "|1100015    |Alta Floresta D'Oeste (RO)|RO |50.94710852687823         |\n",
      "|1100023    |Ariquemes (RO)            |RO |55.97475391330499         |\n",
      "|1100031    |Cabixi (RO)               |RO |51.36453973053614         |\n",
      "|1100049    |Cacoal (RO)               |RO |61.84526595721548         |\n",
      "|1100056    |Cerejeiras (RO)           |RO |58.70878800673873         |\n",
      "+-----------+--------------------------+---+--------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Total de colunas: 79\n",
      "Primeiras 10 colunas: ['C√≥digo IBGE', 'Munic√≠pio', 'UF', '√Årea (km¬≤)', 'Popula√ß√£o 2022', 'PIB per capita 2021', '√çndice de Progresso Social', 'Necessidades Humanas B√°sicas', 'Fundamentos do Bem-estar', 'Oportunidades']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Detecta o caminho do CSV dinamicamente para notebook em /notebooks/\n",
    "# (Sobe 1 n√≠vel de notebooks/ para root do projeto: trabalho_spark/)\n",
    "notebook_dir = os.getcwd()  # Diret√≥rio atual (provavelmente /home/nice_correia/trabalho_spark/notebooks)\n",
    "project_root = os.path.dirname(notebook_dir)  # Sobe 1 n√≠vel: notebooks/ -> trabalho_spark/\n",
    "raw_path = os.path.join(project_root, \"data\", \"ips_brasil.csv\")\n",
    "\n",
    "# Verifica√ß√£o: Mostra caminhos calculados e confirma exist√™ncia\n",
    "print(f\"Diret√≥rio do notebook: {notebook_dir}\")\n",
    "print(f\"Root do projeto detectado: {project_root}\")\n",
    "print(f\"Caminho absoluto do CSV: {raw_path}\")\n",
    "print(f\"Arquivo existe? {os.path.exists(raw_path)}\")\n",
    "\n",
    "# Se n√£o existir (fallback), tenta caminho relativo direto ou absoluto manual\n",
    "if not os.path.exists(raw_path):\n",
    "    # Fallback 1: Se notebook foi aberto de outro lugar, tenta relativo ao cwd\n",
    "    alt_path1 = os.path.join(os.getcwd(), \"data\", \"raw\", \"ips_brasil.csv\")\n",
    "    print(f\"Fallback 1: {alt_path1} (existe? {os.path.exists(alt_path1)})\")\n",
    "    if os.path.exists(alt_path1):\n",
    "        raw_path = alt_path1\n",
    "    else:\n",
    "        # Fallback 2: Caminho absoluto manual (ajuste se o seu home for diferente)\n",
    "        raw_path = \"/home/nice_correia/trabalho_spark/data/raw/ips_brasil.csv\"\n",
    "        print(f\"Fallback 2 (manual): {raw_path} (existe? {os.path.exists(raw_path)})\")\n",
    "        if not os.path.exists(raw_path):\n",
    "            raise FileNotFoundError(f\"CSV n√£o encontrado! Verifique se est√° em {project_root}/data/raw/ips_brasil.csv\")\n",
    "\n",
    "# Carrega o DF com o caminho correto\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(raw_path)\n",
    "print(f\"\\nDados IPS full carregados de '{raw_path}': {df.count()} linhas, {len(df.columns)} colunas.\")\n",
    "\n",
    "# Verifica√ß√£o r√°pida: Schema e top 5 linhas chave (com nomes especiais escapados)\n",
    "print(\"\\nSchema das colunas chave:\")\n",
    "df.select(\"`C√≥digo IBGE`\", \"`Munic√≠pio`\", \"`UF`\", \"`√çndice de Progresso Social`\").printSchema()\n",
    "df.select(\"`C√≥digo IBGE`\", \"`Munic√≠pio`\", \"`UF`\", \"`√çndice de Progresso Social`\").show(5, truncate=False)\n",
    "\n",
    "# Opcional: Lista todas as colunas para confirmar (79 no total)\n",
    "print(f\"\\nTotal de colunas: {len(df.columns)}\")\n",
    "print(\"Primeiras 10 colunas:\", df.columns[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1ae0ee8-a02e-4d25-b2a7-0aff62727b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Iceberg: DDL ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/02 20:00:15 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "25/10/02 20:00:18 WARN HadoopTableOperations: Error reading version hint file file:/tmp/iceberg/default/iceberg_ips/metadata/version-hint.text\n",
      "java.io.FileNotFoundException: File file:/tmp/iceberg/default/iceberg_ips/metadata/version-hint.text does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)\n",
      "\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)\n",
      "\tat org.apache.iceberg.hadoop.HadoopTableOperations.findVersion(HadoopTableOperations.java:318)\n",
      "\tat org.apache.iceberg.hadoop.HadoopTableOperations.refresh(HadoopTableOperations.java:104)\n",
      "\tat org.apache.iceberg.BaseTransaction.lambda$commitReplaceTransaction$1(BaseTransaction.java:368)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)\n",
      "\tat org.apache.iceberg.BaseTransaction.commitReplaceTransaction(BaseTransaction.java:365)\n",
      "\tat org.apache.iceberg.BaseTransaction.commitTransaction(BaseTransaction.java:314)\n",
      "\tat org.apache.iceberg.CommitCallbackTransaction.commitTransaction(CommitCallbackTransaction.java:126)\n",
      "\tat org.apache.iceberg.spark.source.StagedSparkTable.commitStagedChanges(StagedSparkTable.java:34)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:589)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable(WriteToDataSourceV2Exec.scala:579)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable$(WriteToDataSourceV2Exec.scala:572)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:186)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:221)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:645)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:575)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/10/02 20:00:18 WARN HadoopTableOperations: Error reading version hint file file:/tmp/iceberg/default/iceberg_ips/metadata/version-hint.text\n",
      "java.io.FileNotFoundException: File file:/tmp/iceberg/default/iceberg_ips/metadata/version-hint.text does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)\n",
      "\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)\n",
      "\tat org.apache.iceberg.hadoop.HadoopTableOperations.findVersion(HadoopTableOperations.java:318)\n",
      "\tat org.apache.iceberg.hadoop.HadoopTableOperations.refresh(HadoopTableOperations.java:104)\n",
      "\tat org.apache.iceberg.hadoop.HadoopTableOperations.current(HadoopTableOperations.java:84)\n",
      "\tat org.apache.iceberg.BaseTransaction.lambda$commitReplaceTransaction$1(BaseTransaction.java:377)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)\n",
      "\tat org.apache.iceberg.BaseTransaction.commitReplaceTransaction(BaseTransaction.java:365)\n",
      "\tat org.apache.iceberg.BaseTransaction.commitTransaction(BaseTransaction.java:314)\n",
      "\tat org.apache.iceberg.CommitCallbackTransaction.commitTransaction(CommitCallbackTransaction.java:126)\n",
      "\tat org.apache.iceberg.spark.source.StagedSparkTable.commitStagedChanges(StagedSparkTable.java:34)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:589)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable(WriteToDataSourceV2Exec.scala:579)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable$(WriteToDataSourceV2Exec.scala:572)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:186)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:221)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:645)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:575)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabela Iceberg criada com sucesso! (79 colunas com nomes especiais suportados).\n",
      "Total de linhas na tabela: 5570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+---+--------------------------+\n",
      "|C√≥digo IBGE|Munic√≠pio          |UF |√çndice de Progresso Social|\n",
      "+-----------+-------------------+---+--------------------------+\n",
      "|3516853    |Gavi√£o Peixoto (SP)|SP |74.49282395600983         |\n",
      "|5300108    |Bras√≠lia (DF)      |DF |71.25189747327438         |\n",
      "|3548906    |S√£o Carlos (SP)    |SP |70.96059545595044         |\n",
      "|5208707    |Goi√¢nia (GO)       |GO |70.49282747376537         |\n",
      "|3533601    |Nuporanga (SP)     |SP |70.4719550872065          |\n",
      "+-----------+-------------------+---+--------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# === Iceberg: DDL (Cria√ß√£o da Tabela) ===\n",
    "print(\"\\n=== Iceberg: DDL ===\")\n",
    "table_name = \"iceberg_catalog.default.iceberg_ips\"\n",
    "\n",
    "# Limpa se existir\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "\n",
    "# Cria tabela Iceberg a partir do DF\n",
    "df.write.format(\"iceberg\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "\n",
    "print(\"Tabela Iceberg criada com sucesso! (79 colunas com nomes especiais suportados).\")\n",
    "print(f\"Total de linhas na tabela: {spark.sql(f'SELECT COUNT(*) FROM {table_name}').collect()[0][0]}\")\n",
    "\n",
    "# Verifica√ß√£o: Top 5 por IPS\n",
    "spark.sql(f\"SELECT `C√≥digo IBGE`, `Munic√≠pio`, `UF`, `√çndice de Progresso Social` FROM {table_name} ORDER BY `√çndice de Progresso Social` DESC LIMIT 5\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6f01303-cccb-437a-816f-29a10153c528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Iceberg: INSERT ===\n",
      "Total de colunas no DF original: 79\n",
      "Coluna 'C√≥digo IBGE' existe? True\n",
      "Coluna 'Munic√≠pio' existe? True\n",
      "Coluna 'UF' existe? True\n",
      "Coluna '√çndice de Progresso Social' existe? True\n",
      "Coluna 'Popula√ß√£o 2022' existe? True\n",
      "Chaves no data_dict: 79 (deve ser 79)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame fict√≠cio criado: 1 linha(s), 79 colunas.\n",
      "Schema fict√≠cio (primeiras 5 colunas):\n",
      "root\n",
      " |-- C√≥digo IBGE: integer (nullable = true)\n",
      " |-- Munic√≠pio: string (nullable = true)\n",
      " |-- UF: string (nullable = true)\n",
      " |-- √Årea (km¬≤): double (nullable = true)\n",
      " |-- Popula√ß√£o 2022: integer (nullable = true)\n",
      " |-- PIB per capita 2021: double (nullable = true)\n",
      " |-- √çndice de Progresso Social: double (nullable = true)\n",
      " |-- Necessidades Humanas B√°sicas: double (nullable = true)\n",
      " |-- Fundamentos do Bem-estar: double (nullable = true)\n",
      " |-- Oportunidades: double (nullable = true)\n",
      " |-- Nutri√ß√£o e Cuidados M√©dicos B√°sicos: double (nullable = true)\n",
      " |-- √Ågua e Saneamento: double (nullable = true)\n",
      " |-- Moradia: double (nullable = true)\n",
      " |-- Seguran√ßa Pessoal: double (nullable = true)\n",
      " |-- Acesso ao Conhecimento B√°sico: double (nullable = true)\n",
      " |-- Acesso √† Informa√ß√£o e Comunica√ß√£o: double (nullable = true)\n",
      " |-- Sa√∫de e Bem-estar: double (nullable = true)\n",
      " |-- Qualidade do Meio Ambiente: double (nullable = true)\n",
      " |-- Direitos Individuais: double (nullable = true)\n",
      " |-- Liberdades Individuais e de Escolha: double (nullable = true)\n",
      " |-- Inclus√£o Social: double (nullable = true)\n",
      " |-- Acesso √† Educa√ß√£o Superior: double (nullable = true)\n",
      " |-- Cobertura Vacinal (Poliomielite): double (nullable = true)\n",
      " |-- Hospitaliza√ß√µes por Condi√ß√µes Sens√≠veis √† Aten√ß√£o Prim√°ria: double (nullable = true)\n",
      " |-- Mortalidade Ajustada por Condi√ß√µes Sens√≠veis √† Aten√ß√£o Prim√°ria: double (nullable = true)\n",
      " |-- Mortalidade Infantil at√© 5 Anos: double (nullable = true)\n",
      " |-- Subnutri√ß√£o: double (nullable = true)\n",
      " |-- Abastecimento de √Ågua via Rede de Distribui√ß√£o: double (nullable = true)\n",
      " |-- Esgotamento Sanit√°rio Adequado: double (nullable = true)\n",
      " |-- √çndice de Abastecimento de √Ågua: double (nullable = true)\n",
      " |-- √çndice de Perdas de √Ågua na Distribui√ß√£o: double (nullable = true)\n",
      " |-- Domic√≠lios com Coleta de Res√≠duos Adequada: double (nullable = true)\n",
      " |-- Domic√≠lios com Ilumina√ß√£o El√©trica Adequada: double (nullable = true)\n",
      " |-- Domic√≠lios com Paredes Adequadas: double (nullable = true)\n",
      " |-- Domic√≠lios com Piso Adequado: double (nullable = true)\n",
      " |-- Assassinatos de Jovens: double (nullable = true)\n",
      " |-- Assassinatos de Mulheres: double (nullable = true)\n",
      " |-- Homic√≠dios: double (nullable = true)\n",
      " |-- Mortes por Acidente de Transporte: double (nullable = true)\n",
      " |-- Abandono no Ensino Fundamental: double (nullable = true)\n",
      " |-- Abandono no Ensino M√©dio: double (nullable = true)\n",
      " |-- Evas√£o no Ensino M√©dio: double (nullable = true)\n",
      " |-- Distor√ß√£o Idade-S√©rie no Ensino M√©dio: double (nullable = true)\n",
      " |-- Ideb Ensino Fundamental: double (nullable = true)\n",
      " |-- Reprova√ß√£o Escolar no Ensino M√©dio: double (nullable = true)\n",
      " |-- Cobertura de Internet M√≥vel (4G/5G): double (nullable = true)\n",
      " |-- Densidade de Internet Banda Larga Fixa: double (nullable = true)\n",
      " |-- Densidade de Telefonia M√≥vel: double (nullable = true)\n",
      " |-- Qualidade de Internet M√≥vel: double (nullable = true)\n",
      " |-- Consumo de Alimentos Ultraprocessados: string (nullable = true)\n",
      " |-- Expectativa de Vida: double (nullable = true)\n",
      " |-- Mortalidade entre 15 e 50 anos: double (nullable = true)\n",
      " |-- Mortalidade por Doen√ßas Cr√¥nicas N√£o Transmiss√≠veis: double (nullable = true)\n",
      " |-- Obesidade: double (nullable = true)\n",
      " |-- Suic√≠dios: double (nullable = true)\n",
      " |-- √Åreas Verdes Urbanas: double (nullable = true)\n",
      " |-- Emiss√µes de CO‚ÇÇe por Habitante: double (nullable = true)\n",
      " |-- Focos de Calor: double (nullable = true)\n",
      " |-- √çndice de Vulnerabilidade Clim√°tica dos Munic√≠pios (IVCM): double (nullable = true)\n",
      " |-- Supress√£o da Vegeta√ß√£o Prim√°ria e Secund√°ria: double (nullable = true)\n",
      " |-- Acesso a Programas de Direitos Humanos: integer (nullable = true)\n",
      " |-- Exist√™ncia de A√ß√µes para Direitos de Minorias: integer (nullable = true)\n",
      " |-- √çndice de Atendimento √† Demanda de Justi√ßa: double (nullable = true)\n",
      " |-- Resposta a Processos Previdenci√°rios: string (nullable = true)\n",
      " |-- Resposta a Processos Familiares: string (nullable = true)\n",
      " |-- Taxa de Congestionamento L√≠quido de Processos: double (nullable = true)\n",
      " |-- Acesso √† Cultura, Lazer e Esporte: integer (nullable = true)\n",
      " |-- Gravidez na Adolesc√™ncia (<19 anos): double (nullable = true)\n",
      " |-- √çndice de Vulnerabilidade das Fam√≠lias do Cadastro √önico (IVCAD): string (nullable = true)\n",
      " |-- Pra√ßas e Parques em √Åreas Urbanas: double (nullable = true)\n",
      " |-- Fam√≠lias em Situa√ß√£o de Rua: string (nullable = true)\n",
      " |-- Paridade de G√™nero na C√¢mara Municipal: double (nullable = true)\n",
      " |-- Paridade de Negros na C√¢mara Municipal: double (nullable = true)\n",
      " |-- Viol√™ncia contra Ind√≠genas: double (nullable = true)\n",
      " |-- Viol√™ncia contra Mulheres: double (nullable = true)\n",
      " |-- Viol√™ncia contra Negros: double (nullable = true)\n",
      " |-- Empregados com Ensino Superior: double (nullable = true)\n",
      " |-- Mulheres Empregadas com Ensino Superior: double (nullable = true)\n",
      " |-- Nota Mediana do Enem: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSERT: Linha fict√≠cia adicionada com sucesso! (Nova snapshot criada no Iceberg).\n",
      "+-----------+-------------------+---+--------------------------+\n",
      "|C√≥digo IBGE|Munic√≠pio          |UF |√çndice de Progresso Social|\n",
      "+-----------+-------------------+---+--------------------------+\n",
      "|9999999    |Exemplo Fict√≠cio   |XX |99.9                      |\n",
      "|3516853    |Gavi√£o Peixoto (SP)|SP |74.49282395600983         |\n",
      "|5300108    |Bras√≠lia (DF)      |DF |71.25189747327438         |\n",
      "|3548906    |S√£o Carlos (SP)    |SP |70.96059545595044         |\n",
      "|5208707    |Goi√¢nia (GO)       |GO |70.49282747376537         |\n",
      "+-----------+-------------------+---+--------------------------+\n",
      "\n",
      "Total de linhas ap√≥s INSERT: 5571\n",
      "+-----------+----------------+---+--------------------------+--------------+\n",
      "|C√≥digo IBGE|Munic√≠pio       |UF |√çndice de Progresso Social|Popula√ß√£o 2022|\n",
      "+-----------+----------------+---+--------------------------+--------------+\n",
      "|9999999    |Exemplo Fict√≠cio|XX |99.9                      |100000        |\n",
      "+-----------+----------------+---+--------------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# === Iceberg: INSERT (Adi√ß√£o de Dados) - VERS√ÉO FINAL CORRIGIDA ===\n",
    "print(\"\\n=== Iceberg: INSERT ===\")\n",
    "table_name = \"iceberg_catalog.default.iceberg_ips\"\n",
    "\n",
    "# Verifica√ß√£o inicial: Confirma colunas chave (baseado no seu df.columns)\n",
    "print(f\"Total de colunas no DF original: {len(df.columns)}\")\n",
    "key_columns = [\"C√≥digo IBGE\", \"Munic√≠pio\", \"UF\", \"√çndice de Progresso Social\", \"Popula√ß√£o 2022\"]\n",
    "for col in key_columns:\n",
    "    exists = col in df.columns\n",
    "    print(f\"Coluna '{col}' existe? {exists}\")\n",
    "    if not exists:\n",
    "        print(f\"AVISO: Coluna '{col}' n√£o encontrada! Colunas similares: {[c for c in df.columns if col.lower() in c.lower()]}\")\n",
    "\n",
    "# Cria DataFrame fict√≠cio com schema EXATO do original\n",
    "from pyspark.sql import Row\n",
    "data_dict = {col: None for col in df.columns}  # 79 chaves, todas NULL\n",
    "\n",
    "# Define s√≥ colunas que existem (evita erros)\n",
    "if \"C√≥digo IBGE\" in df.columns:\n",
    "    data_dict[\"C√≥digo IBGE\"] = 9999999  # Fict√≠cio (int)\n",
    "if \"Munic√≠pio\" in df.columns:\n",
    "    data_dict[\"Munic√≠pio\"] = \"Exemplo Fict√≠cio\"  # String\n",
    "if \"UF\" in df.columns:\n",
    "    data_dict[\"UF\"] = \"XX\"  # String\n",
    "if \"√çndice de Progresso Social\" in df.columns:\n",
    "    data_dict[\"√çndice de Progresso Social\"] = 99.9  # Double alto para topo\n",
    "if \"Popula√ß√£o 2022\" in df.columns:\n",
    "    data_dict[\"Popula√ß√£o 2022\"] = 100000  # Int/long fict√≠cio (usa coluna real do seu dataset)\n",
    "\n",
    "# Confirma: Deve ter exatamente 79 chaves\n",
    "print(f\"Chaves no data_dict: {len(data_dict)} (deve ser {len(df.columns)})\")\n",
    "\n",
    "# Cria Row e DataFrame com SCHEMA EXPL√çCITO (resolve infer√™ncia com NULLs)\n",
    "fictitious_row = Row(**data_dict)\n",
    "fictitious_df = spark.createDataFrame([fictitious_row], df.schema)  # <- CHAVE: Usa df.schema para tipos exatos\n",
    "print(f\"DataFrame fict√≠cio criado: {fictitious_df.count()} linha(s), {len(fictitious_df.columns)} colunas.\")\n",
    "print(\"Schema fict√≠cio (primeiras 5 colunas):\")\n",
    "fictitious_df.printSchema()  # Mostra tipos inferidos/for√ßados (ex: double para IPS)\n",
    "\n",
    "# INSERT via append (adiciona ao cat√°logo Iceberg - snapshot novo criado)\n",
    "fictitious_df.write.format(\"iceberg\").mode(\"append\").saveAsTable(table_name)  # Use saveAsTable para compatibilidade, ou insertInto\n",
    "\n",
    "print(\"INSERT: Linha fict√≠cia adicionada com sucesso! (Nova snapshot criada no Iceberg).\")\n",
    "\n",
    "# Verifica√ß√£o: Top 5 linhas ordenadas por IPS (deve incluir a nova no topo)\n",
    "spark.sql(f\"SELECT `C√≥digo IBGE`, `Munic√≠pio`, `UF`, `√çndice de Progresso Social` FROM {table_name} ORDER BY `√çndice de Progresso Social` DESC LIMIT 5\").show(truncate=False)\n",
    "\n",
    "# Conta total (deve ser original +1 = ~5566)\n",
    "total_rows = spark.sql(f'SELECT COUNT(*) FROM {table_name}').collect()[0][0]\n",
    "print(f\"Total de linhas ap√≥s INSERT: {total_rows}\")\n",
    "\n",
    "# Verifica√ß√£o extra: Mostra a linha fict√≠cia inserida (com Popula√ß√£o 2022)\n",
    "spark.sql(f\"SELECT `C√≥digo IBGE`, `Munic√≠pio`, `UF`, `√çndice de Progresso Social`, `Popula√ß√£o 2022` FROM {table_name} WHERE `C√≥digo IBGE` = 9999999\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "772e3f29-c29f-4fe3-8a0b-e1b8ba2c6752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+-----------------------+\n",
      "|snapshot_id        |operation|committed_at           |\n",
      "+-------------------+---------+-----------------------+\n",
      "|1260496121539592792|append   |2025-10-02 20:00:39.085|\n",
      "|3870061069435861093|overwrite|2025-10-02 20:00:18.681|\n",
      "+-------------------+---------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verifica√ß√£o de snapshot ap√≥s INSERT\n",
    "snapshots_df = spark.sql(f\"SELECT snapshot_id, operation, committed_at FROM {table_name}.snapshots ORDER BY committed_at DESC LIMIT 2\")\n",
    "snapshots_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d6c531d-9715-4bc2-a270-a777a4cb30f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Iceberg: UPDATE ===\n",
      "Verificando C√≥digo IBGE de S√£o Paulo...\n",
      "+-----------+--------------------------+---+--------------------------+\n",
      "|C√≥digo IBGE|Munic√≠pio                 |UF |√çndice de Progresso Social|\n",
      "+-----------+--------------------------+---+--------------------------+\n",
      "|1303908    |S√£o Paulo de Oliven√ßa (AM)|AM |48.6949936570276          |\n",
      "+-----------+--------------------------+---+--------------------------+\n",
      "\n",
      "S√£o Paulo encontrado: C√≥digo 1303908, S√£o Paulo de Oliven√ßa (AM), AM\n",
      "DataFrame de UPDATE criado com sucesso (schema program√°tico):\n",
      "+-----------+--------------------------+---+--------------------------+\n",
      "|C√≥digo IBGE|Munic√≠pio                 |UF |√çndice de Progresso Social|\n",
      "+-----------+--------------------------+---+--------------------------+\n",
      "|1303908    |S√£o Paulo de Oliven√ßa (AM)|AM |70.5                      |\n",
      "+-----------+--------------------------+---+--------------------------+\n",
      "\n",
      "Schema: 4 colunas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATE: IPS de S√£o Paulo de Oliven√ßa (AM) atualizado para 70.5! (MERGE ACID executado - nova snapshot criada).\n",
      "+-----------+--------------------------+---+--------------------------+\n",
      "|C√≥digo IBGE|Munic√≠pio                 |UF |√çndice de Progresso Social|\n",
      "+-----------+--------------------------+---+--------------------------+\n",
      "|1303908    |S√£o Paulo de Oliven√ßa (AM)|AM |70.5                      |\n",
      "+-----------+--------------------------+---+--------------------------+\n",
      "\n",
      "+-----------+--------------------------+---+--------------------------+\n",
      "|C√≥digo IBGE|Munic√≠pio                 |UF |√çndice de Progresso Social|\n",
      "+-----------+--------------------------+---+--------------------------+\n",
      "|9999999    |Exemplo Fict√≠cio          |XX |99.9                      |\n",
      "|3516853    |Gavi√£o Peixoto (SP)       |SP |74.49282395600983         |\n",
      "|5300108    |Bras√≠lia (DF)             |DF |71.25189747327438         |\n",
      "|3548906    |S√£o Carlos (SP)           |SP |70.96059545595044         |\n",
      "|1303908    |S√£o Paulo de Oliven√ßa (AM)|AM |70.5                      |\n",
      "+-----------+--------------------------+---+--------------------------+\n",
      "\n",
      "Total de linhas ap√≥s UPDATE: 5571 (mesmo total, s√≥ atualizado)\n"
     ]
    }
   ],
   "source": [
    "# === Iceberg: UPDATE (Atualiza√ß√£o de Dados) - VERS√ÉO CORRIGIDA ===\n",
    "print(\"\\n=== Iceberg: UPDATE ===\")\n",
    "table_name = \"iceberg_catalog.default.iceberg_ips\"\n",
    "\n",
    "# Verifica√ß√£o: Confirma se C√≥digo IBGE 3550308 (S√£o Paulo) existe no dataset\n",
    "print(\"Verificando C√≥digo IBGE de S√£o Paulo...\")\n",
    "saopaulo_check = spark.sql(f\"SELECT `C√≥digo IBGE`, `Munic√≠pio`, `UF`, `√çndice de Progresso Social` FROM {table_name} WHERE `Munic√≠pio` LIKE '%S√£o Paulo%' LIMIT 1\")\n",
    "saopaulo_check.show(truncate=False)\n",
    "if saopaulo_check.count() == 0:\n",
    "    print(\"AVISO: S√£o Paulo n√£o encontrado! Use outro munic√≠pio. Exemplo: Pegue o primeiro da tabela.\")\n",
    "    # Fallback: Use o primeiro munic√≠pio da tabela como exemplo\n",
    "    first_city = spark.sql(f\"SELECT `C√≥digo IBGE`, `Munic√≠pio`, `UF` FROM {table_name} LIMIT 1\").collect()[0]\n",
    "    codigo_ibge = first_city[0]\n",
    "    municipio = first_city[1]\n",
    "    uf = first_city[2]\n",
    "    print(f\"Fallback: Usando {municipio} (C√≥digo {codigo_ibge}) para UPDATE.\")\n",
    "else:\n",
    "    row = saopaulo_check.collect()[0]\n",
    "    codigo_ibge = row[0]\n",
    "    municipio = row[1]\n",
    "    uf = row[2]\n",
    "    print(f\"S√£o Paulo encontrado: C√≥digo {codigo_ibge}, {municipio}, {uf}\")\n",
    "\n",
    "# DataFrame para UPDATE: Usa o c√≥digo real encontrado e atualiza IPS para 70.5 (fict√≠cio)\n",
    "update_data = [(codigo_ibge, municipio, uf, 70.5)]  # Novo IPS mais alto\n",
    "\n",
    "# Schema program√°tico com StructType (resolve acentos - sem string DDL)\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "update_schema = StructType([\n",
    "    StructField(\"C√≥digo IBGE\", IntegerType(), True),  # int, nullable\n",
    "    StructField(\"Munic√≠pio\", StringType(), True),     # string\n",
    "    StructField(\"UF\", StringType(), True),            # string\n",
    "    StructField(\"√çndice de Progresso Social\", DoubleType(), True)  # double (com acento OK!)\n",
    "])\n",
    "update_df = spark.createDataFrame(update_data, update_schema)\n",
    "print(\"DataFrame de UPDATE criado com sucesso (schema program√°tico):\")\n",
    "update_df.show(truncate=False)  # Mostra a linha de update\n",
    "print(f\"Schema: {len(update_df.columns)} colunas\")\n",
    "\n",
    "# Cria temp view para o MERGE\n",
    "update_df.createOrReplaceTempView(\"update_temp\")\n",
    "\n",
    "# UPDATE via MERGE INTO (ACID do Iceberg - atualiza baseado em C√≥digo IBGE)\n",
    "spark.sql(f\"\"\"\n",
    "    MERGE INTO {table_name} AS target\n",
    "    USING update_temp AS source\n",
    "    ON target.`C√≥digo IBGE` = source.`C√≥digo IBGE`\n",
    "    WHEN MATCHED THEN \n",
    "        UPDATE SET \n",
    "            target.`√çndice de Progresso Social` = source.`√çndice de Progresso Social`,\n",
    "            target.`Munic√≠pio` = source.`Munic√≠pio`,\n",
    "            target.`UF` = source.`UF`\n",
    "\"\"\")\n",
    "\n",
    "print(f\"UPDATE: IPS de {municipio} atualizado para 70.5! (MERGE ACID executado - nova snapshot criada).\")\n",
    "\n",
    "# Verifica√ß√£o: Mostra a linha atualizada (deve ter IPS = 70.5)\n",
    "spark.sql(f\"SELECT `C√≥digo IBGE`, `Munic√≠pio`, `UF`, `√çndice de Progresso Social` FROM {table_name} WHERE `C√≥digo IBGE` = {codigo_ibge}\").show(truncate=False)\n",
    "\n",
    "# Top 5 atualizado (o munic√≠pio deve subir no ranking)\n",
    "spark.sql(f\"SELECT `C√≥digo IBGE`, `Munic√≠pio`, `UF`, `√çndice de Progresso Social` FROM {table_name} ORDER BY `√çndice de Progresso Social` DESC LIMIT 5\").show(truncate=False)\n",
    "\n",
    "# Conta total (deve ser o mesmo, s√≥ atualizado)\n",
    "total_rows = spark.sql(f'SELECT COUNT(*) FROM {table_name}').collect()[0][0]\n",
    "print(f\"Total de linhas ap√≥s UPDATE: {total_rows} (mesmo total, s√≥ atualizado)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da8e391a-ca5a-4316-b59f-80efeea44bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Iceberg: Time Travel ===\n",
      "+-----------------------+-------------------+-------------------+---------+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|committed_at           |snapshot_id        |parent_id          |operation|manifest_list                                                                                                      |summary                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "+-----------------------+-------------------+-------------------+---------+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|2025-10-02 20:01:06.469|4929695402238117548|1260496121539592792|overwrite|file:/tmp/iceberg/default/iceberg_ips/metadata/snap-4929695402238117548-1-c50138ea-ed00-4d7e-b6e5-d828eca1cb71.avro|{spark.app.id -> local-1759443727856, added-data-files -> 2, deleted-data-files -> 1, added-records -> 4215, deleted-records -> 4215, added-files-size -> 1883154, removed-files-size -> 1806817, changed-partition-count -> 1, total-records -> 5571, total-files-size -> 2503121, total-data-files -> 4, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}|\n",
      "|2025-10-02 20:00:39.085|1260496121539592792|3870061069435861093|append   |file:/tmp/iceberg/default/iceberg_ips/metadata/snap-1260496121539592792-1-051b4e6a-fadc-45f5-88dd-d4f782e7161d.avro|{spark.app.id -> local-1759443727856, added-data-files -> 1, added-records -> 1, added-files-size -> 23511, changed-partition-count -> 1, total-records -> 5571, total-files-size -> 2426784, total-data-files -> 3, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}                                                                                      |\n",
      "|2025-10-02 20:00:18.681|3870061069435861093|NULL               |overwrite|file:/tmp/iceberg/default/iceberg_ips/metadata/snap-3870061069435861093-1-3067792e-764e-458e-8efe-ddfa451a290a.avro|{spark.app.id -> local-1759443727856, added-data-files -> 2, added-records -> 5570, added-files-size -> 2403273, changed-partition-count -> 1, total-records -> 5570, total-files-size -> 2403273, total-data-files -> 2, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}                                                                                 |\n",
      "+-----------------------+-------------------+-------------------+---------+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "+-------------------+-------------------+---------+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|snapshot_id        |parent_id          |operation|manifest_list                                                                                                      |summary                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "+-------------------+-------------------+---------+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|4929695402238117548|1260496121539592792|overwrite|file:/tmp/iceberg/default/iceberg_ips/metadata/snap-4929695402238117548-1-c50138ea-ed00-4d7e-b6e5-d828eca1cb71.avro|{spark.app.id -> local-1759443727856, added-data-files -> 2, deleted-data-files -> 1, added-records -> 4215, deleted-records -> 4215, added-files-size -> 1883154, removed-files-size -> 1806817, changed-partition-count -> 1, total-records -> 5571, total-files-size -> 2503121, total-data-files -> 4, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}|\n",
      "|1260496121539592792|3870061069435861093|append   |file:/tmp/iceberg/default/iceberg_ips/metadata/snap-1260496121539592792-1-051b4e6a-fadc-45f5-88dd-d4f782e7161d.avro|{spark.app.id -> local-1759443727856, added-data-files -> 1, added-records -> 1, added-files-size -> 23511, changed-partition-count -> 1, total-records -> 5571, total-files-size -> 2426784, total-data-files -> 3, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}                                                                                      |\n",
      "|3870061069435861093|NULL               |overwrite|file:/tmp/iceberg/default/iceberg_ips/metadata/snap-3870061069435861093-1-3067792e-764e-458e-8efe-ddfa451a290a.avro|{spark.app.id -> local-1759443727856, added-data-files -> 2, added-records -> 5570, added-files-size -> 2403273, changed-partition-count -> 1, total-records -> 5570, total-files-size -> 2403273, total-data-files -> 2, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}                                                                                 |\n",
      "+-------------------+-------------------+---------+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "\n",
      "Vers√£o ANTES das mudan√ßas (snapshot mais antigo ID 3870061069435861093, opera√ß√£o: overwrite):\n",
      "+-----------+-------------------+---+--------------------------+\n",
      "|C√≥digo IBGE|Munic√≠pio          |UF |√çndice de Progresso Social|\n",
      "+-----------+-------------------+---+--------------------------+\n",
      "|3516853    |Gavi√£o Peixoto (SP)|SP |74.49282395600983         |\n",
      "|5300108    |Bras√≠lia (DF)      |DF |71.25189747327438         |\n",
      "|3548906    |S√£o Carlos (SP)    |SP |70.96059545595044         |\n",
      "+-----------+-------------------+---+--------------------------+\n",
      "\n",
      "Linhas na vers√£o antiga: 5570 (sem INSERT/UPDATE - s√≥ dados originais do DDL)\n",
      "\n",
      "Vers√£o ATUAL (snapshot mais recente, ap√≥s DDL/INSERT/UPDATE):\n",
      "+-----------+-------------------+---+--------------------------+\n",
      "|C√≥digo IBGE|Munic√≠pio          |UF |√çndice de Progresso Social|\n",
      "+-----------+-------------------+---+--------------------------+\n",
      "|9999999    |Exemplo Fict√≠cio   |XX |99.9                      |\n",
      "|3516853    |Gavi√£o Peixoto (SP)|SP |74.49282395600983         |\n",
      "|5300108    |Bras√≠lia (DF)      |DF |71.25189747327438         |\n",
      "+-----------+-------------------+---+--------------------------+\n",
      "\n",
      "Linhas na vers√£o atual: 5571 (com INSERT + UPDATE aplicado)\n",
      "Diferen√ßa de linhas (atual vs. antigo): +1 (devido ao INSERT; UPDATE n√£o altera contagem)\n",
      "\n",
      "Time Travel completo! Iceberg mant√©m hist√≥rico imut√°vel para auditoria e rollback.\n"
     ]
    }
   ],
   "source": [
    "# === Iceberg: Time Travel (Snapshots e Hist√≥rico) - VERS√ÉO CORRIGIDA ===\n",
    "print(\"\\n=== Iceberg: Time Travel ===\")\n",
    "table_name = \"iceberg_catalog.default.iceberg_ips\"\n",
    "\n",
    "# Lista TODOS os snapshots (hist√≥rico de mudan√ßas - deve ter 3+: DDL, INSERT, UPDATE)\n",
    "snapshots_df = spark.sql(f\"SELECT * FROM {table_name}.snapshots ORDER BY committed_at DESC\")\n",
    "#print(f\"Total de snapshots encontrados: {snapshots_df.count()} (esperado: 3+ ap√≥s DDL/INSERT/UPDATE)\")\n",
    "#print(\"Snapshots recentes (hist√≥rico completo):\")\n",
    "snapshots_df.show(truncate=False, n=10)  # Mostra todas as colunas e linhas (√∫til para debug)\n",
    "\n",
    "# Select customizado com NOMES CORRETOS de colunas (parent_id, n√£o parent_snapshot_id)\n",
    "#print(\"\\nSnapshots selecionados (colunas chave):\")\n",
    "snapshots_df.select(\"snapshot_id\", \"parent_id\", \"operation\", \"manifest_list\", \"summary\").show(truncate=False, n=5)\n",
    "\n",
    "# Time Travel: Consulta vers√£o ANTES do INSERT/UPDATE (snapshot mais antigo, se existir m√∫ltiplos)\n",
    "if snapshots_df.count() > 1:\n",
    "    # Pega o snapshot mais antigo (√∫ltima linha ap√≥s ORDER BY DESC)\n",
    "    snapshots_list = snapshots_df.collect()  # Coleta todos (poucos snapshots, OK)\n",
    "    old_snapshot_id = snapshots_list[-1][1]  # snapshot_id √© a 2¬™ coluna (√≠ndice 1: committed_at=0, snapshot_id=1)\n",
    "    old_operation = snapshots_list[-1][3] if len(snapshots_list[-1]) > 3 else \"unknown\"  # operation\n",
    "    print(f\"\\nVers√£o ANTES das mudan√ßas (snapshot mais antigo ID {old_snapshot_id}, opera√ß√£o: {old_operation}):\")\n",
    "    spark.sql(f\"SELECT `C√≥digo IBGE`, `Munic√≠pio`, `UF`, `√çndice de Progresso Social` FROM {table_name} VERSION AS OF {old_snapshot_id} ORDER BY `√çndice de Progresso Social` DESC LIMIT 3\").show(truncate=False)\n",
    "    old_count = spark.sql(f\"SELECT COUNT(*) FROM {table_name} VERSION AS OF {old_snapshot_id}\").collect()[0][0]\n",
    "    print(f\"Linhas na vers√£o antiga: {old_count} (sem INSERT/UPDATE - s√≥ dados originais do DDL)\")\n",
    "else:\n",
    "    print(\"\\nApenas 1 snapshot (s√≥ DDL) - pulando Time Travel antigo. Rode mais DMLs para hist√≥rico.\")\n",
    "\n",
    "# Time Travel: Vers√£o ATUAL (ap√≥s todos os DML - deve incluir INSERT fict√≠cio e UPDATE em S√£o Paulo)\n",
    "print(f\"\\nVers√£o ATUAL (snapshot mais recente, ap√≥s DDL/INSERT/UPDATE):\")\n",
    "current_snapshot_id = snapshots_df.collect()[0][1]  # Primeiro da lista (mais recente)\n",
    "spark.sql(f\"SELECT `C√≥digo IBGE`, `Munic√≠pio`, `UF`, `√çndice de Progresso Social` FROM {table_name} VERSION AS OF {current_snapshot_id} ORDER BY `√çndice de Progresso Social` DESC LIMIT 3\").show(truncate=False)\n",
    "current_count = spark.sql(f'SELECT COUNT(*) FROM {table_name}').collect()[0][0]\n",
    "print(f\"Linhas na vers√£o atual: {current_count} (com INSERT + UPDATE aplicado)\")\n",
    "\n",
    "# Diferen√ßa: Compara contagens (deve mostrar impacto do INSERT)\n",
    "if snapshots_df.count() > 1:\n",
    "    diff = current_count - old_count\n",
    "    print(f\"Diferen√ßa de linhas (atual vs. antigo): +{diff} (devido ao INSERT; UPDATE n√£o altera contagem)\")\n",
    "\n",
    "# Opcional: Rollback para snapshot anterior (ex: desfaz UPDATE/INSERT - COMENTADO por seguran√ßa)\n",
    "# if old_snapshot_id:\n",
    "#     spark.sql(f\"CALL iceberg_system.rollback_to_snapshot('{table_name}', {old_snapshot_id})\")\n",
    "#     print(f\"Rollback executado para snapshot {old_snapshot_id}! (Desfaz mudan√ßas - verifique com SELECT COUNT(*))\")\n",
    "\n",
    "print(\"\\nTime Travel completo! Iceberg mant√©m hist√≥rico imut√°vel para auditoria e rollback.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8de9a8b-756d-45bb-8d55-d38d6a689ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Iceberg: DELETE ===\n",
      "DELETE: Linha fict√≠cia removida! (DELETE ACID executado - nova snapshot).\n",
      "+-----------+---------+---+--------------------------+\n",
      "|C√≥digo IBGE|Munic√≠pio|UF |√çndice de Progresso Social|\n",
      "+-----------+---------+---+--------------------------+\n",
      "+-----------+---------+---+--------------------------+\n",
      "\n",
      "+-----------+--------------------------+---+--------------------------+\n",
      "|C√≥digo IBGE|Munic√≠pio                 |UF |√çndice de Progresso Social|\n",
      "+-----------+--------------------------+---+--------------------------+\n",
      "|3516853    |Gavi√£o Peixoto (SP)       |SP |74.49282395600983         |\n",
      "|5300108    |Bras√≠lia (DF)             |DF |71.25189747327438         |\n",
      "|3548906    |S√£o Carlos (SP)           |SP |70.96059545595044         |\n",
      "|1303908    |S√£o Paulo de Oliven√ßa (AM)|AM |70.5                      |\n",
      "|5208707    |Goi√¢nia (GO)              |GO |70.49282747376537         |\n",
      "+-----------+--------------------------+---+--------------------------+\n",
      "\n",
      "Total de linhas ap√≥s DELETE: 5570 (original -1 = ~5565)\n"
     ]
    }
   ],
   "source": [
    "# === Iceberg: DELETE (Remo√ß√£o de Dados) ===\n",
    "print(\"\\n=== Iceberg: DELETE ===\")\n",
    "table_name = \"iceberg_catalog.default.iceberg_ips\"\n",
    "\n",
    "# DELETE via SQL (remove a linha fict√≠cia INSERTed e, opcionalmente, outra condi√ß√£o)\n",
    "spark.sql(f\"DELETE FROM {table_name} WHERE `C√≥digo IBGE` = 9999999\")  # Remove a fict√≠cia\n",
    "# Opcional: spark.sql(f\"DELETE FROM {table_name} WHERE `UF` = 'XX'\")  # Remove todas de UF fict√≠cia\n",
    "\n",
    "print(\"DELETE: Linha fict√≠cia removida! (DELETE ACID executado - nova snapshot).\")\n",
    "\n",
    "# Verifica√ß√£o: Busca a linha deletada (deve retornar vazio)\n",
    "spark.sql(f\"SELECT `C√≥digo IBGE`, `Munic√≠pio`, `UF`, `√çndice de Progresso Social` FROM {table_name} WHERE `C√≥digo IBGE` = 9999999\").show(truncate=False)\n",
    "\n",
    "# Top 5 atualizado (fict√≠cia sumiu, S√£o Paulo ainda no topo)\n",
    "spark.sql(f\"SELECT `C√≥digo IBGE`, `Munic√≠pio`, `UF`, `√çndice de Progresso Social` FROM {table_name} ORDER BY `√çndice de Progresso Social` DESC LIMIT 5\").show(truncate=False)\n",
    "\n",
    "total_rows = spark.sql(f'SELECT COUNT(*) FROM {table_name}').collect()[0][0]\n",
    "print(f\"Total de linhas ap√≥s DELETE: {total_rows} (original -1 = ~5565)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbea2ee6-fa98-4d48-9887-a9914fdd83ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Iceberg: Time Travel ===\n",
      "Total de snapshots encontrados: 4 (esperado: 4+ ap√≥s DDL/INSERT/UPDATE/DELETE)\n",
      "Snapshots recentes (hist√≥rico completo - operation e summary chave):\n",
      "+-------------------+-------------------+-----------------------+---------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------+\n",
      "|snapshot_id        |parent_id          |committed_at           |operation|summary                                                                                                                                                                                                                                                                                                                                                                                      |manifest_list                                                                                                      |\n",
      "+-------------------+-------------------+-----------------------+---------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------+\n",
      "|6667416611466282884|5231231222819707019|2025-10-01 23:03:39.073|delete   |{spark.app.id -> local-1759370374697, deleted-data-files -> 1, deleted-records -> 1, removed-files-size -> 23511, changed-partition-count -> 1, total-records -> 5570, total-files-size -> 2479610, total-data-files -> 3, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}                                                                                |file:/tmp/iceberg/default/iceberg_ips/metadata/snap-6667416611466282884-1-28414cac-6036-4221-861b-dfce93fb68a4.avro|\n",
      "|5231231222819707019|5960339506171772936|2025-10-01 23:00:43.652|overwrite|{spark.app.id -> local-1759370374697, added-data-files -> 2, deleted-data-files -> 1, added-records -> 4215, deleted-records -> 4215, added-files-size -> 1883154, removed-files-size -> 1806817, changed-partition-count -> 1, total-records -> 5571, total-files-size -> 2503121, total-data-files -> 4, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}|file:/tmp/iceberg/default/iceberg_ips/metadata/snap-5231231222819707019-1-e9a0b43c-5f02-4a74-bd75-010d9378ff78.avro|\n",
      "|5960339506171772936|6605234926047302523|2025-10-01 23:00:28.29 |append   |{spark.app.id -> local-1759370374697, added-data-files -> 1, added-records -> 1, added-files-size -> 23511, changed-partition-count -> 1, total-records -> 5571, total-files-size -> 2426784, total-data-files -> 3, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}                                                                                      |file:/tmp/iceberg/default/iceberg_ips/metadata/snap-5960339506171772936-1-eddeeb12-d164-4ba4-a2db-28e011e33856.avro|\n",
      "|6605234926047302523|NULL               |2025-10-01 23:00:08.921|overwrite|{spark.app.id -> local-1759370374697, added-data-files -> 2, added-records -> 5570, added-files-size -> 2403273, changed-partition-count -> 1, total-records -> 5570, total-files-size -> 2403273, total-data-files -> 2, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}                                                                                 |file:/tmp/iceberg/default/iceberg_ips/metadata/snap-6605234926047302523-1-e851a0c6-6d08-4ca9-af03-0ff0a05a0d94.avro|\n",
      "+-------------------+-------------------+-----------------------+---------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "\n",
      "1. Vers√£o INICIAL (snapshot 6605234926047302523 - ap√≥s DDL, dados originais):\n",
      "+-----------+--------------+---+--------------------------+\n",
      "|C√≥digo IBGE|Munic√≠pio     |UF |√çndice de Progresso Social|\n",
      "+-----------+--------------+---+--------------------------+\n",
      "|3550308    |S√£o Paulo (SP)|SP |68.78752704301019         |\n",
      "+-----------+--------------+---+--------------------------+\n",
      "\n",
      "   Linhas totais: 5570 (CSV original)\n",
      "   Observa√ß√£o: Sem fict√≠cia (9999999); IPS original de S√£o Paulo (~66)\n",
      "\n",
      "2. Ap√≥s INSERT (snapshot 5231231222819707019 - fict√≠cia adicionada):\n",
      "+-----------+----------------+---+--------------------------+\n",
      "|C√≥digo IBGE|Munic√≠pio       |UF |√çndice de Progresso Social|\n",
      "+-----------+----------------+---+--------------------------+\n",
      "|9999999    |Exemplo Fict√≠cio|XX |99.9                      |\n",
      "|3550308    |S√£o Paulo (SP)  |SP |68.78752704301019         |\n",
      "+-----------+----------------+---+--------------------------+\n",
      "\n",
      "   Linhas totais: 5571 (+1 da fict√≠cia 9999999 com IPS 99.9 no topo)\n",
      "\n",
      "3. Ap√≥s UPDATE, ANTES DELETE (snapshot 5960339506171772936 - UPDATE aplicado):\n",
      "+-----------+----------------+---+--------------------------+\n",
      "|C√≥digo IBGE|Munic√≠pio       |UF |√çndice de Progresso Social|\n",
      "+-----------+----------------+---+--------------------------+\n",
      "|9999999    |Exemplo Fict√≠cio|XX |99.9                      |\n",
      "|3550308    |S√£o Paulo (SP)  |SP |68.78752704301019         |\n",
      "+-----------+----------------+---+--------------------------+\n",
      "\n",
      "   Linhas totais: 5571 (mesmo que ap√≥s INSERT; IPS de S√£o Paulo = 70.5)\n",
      "   Observa√ß√£o: Fict√≠cia (9999999) ainda existe; S√£o Paulo atualizado\n",
      "\n",
      "4. Ap√≥s DELETE (snapshot 6667416611466282884 - atual, fict√≠cia removida):\n",
      "+-----------+--------------+---+--------------------------+\n",
      "|C√≥digo IBGE|Munic√≠pio     |UF |√çndice de Progresso Social|\n",
      "+-----------+--------------+---+--------------------------+\n",
      "|3550308    |S√£o Paulo (SP)|SP |68.78752704301019         |\n",
      "+-----------+--------------+---+--------------------------+\n",
      "\n",
      "   Linhas totais: 5570 (-1 da fict√≠cia; volta ao inicial)\n",
      "   Observa√ß√£o: Fict√≠cia (9999999) sumiu; S√£o Paulo mant√©m IPS 70.5 (UPDATE intacto)\n",
      "\n",
      "Resumo de Mudan√ßas:\n",
      "   Inicial (snapshot 6605234926047302523): 5570 linhas\n",
      "   Ap√≥s INSERT (snapshot 5231231222819707019): 5571 linhas (+1)\n",
      "   Ap√≥s UPDATE (snapshot 5960339506171772936): 5571 linhas (sem mudan√ßa)\n",
      "   Ap√≥s DELETE (snapshot 6667416611466282884): 5570 linhas (-1)\n",
      "\n",
      "Time Travel Iceberg completo! Hist√≥rico imut√°vel - INSERT/UPDATE/DELETE audit√°veis em snapshots espec√≠ficos.\n"
     ]
    }
   ],
   "source": [
    "# === Iceberg: Time Travel (Completo: Antes/Depois de INSERT, UPDATE e DELETE) ===\n",
    "print(\"\\n=== Iceberg: Time Travel ===\")\n",
    "table_name = \"iceberg_catalog.default.iceberg_ips\"\n",
    "\n",
    "# Lista TODOS os snapshots (hist√≥rico de mudan√ßas - ORDER BY DESC para mais recente no topo)\n",
    "snapshots_df = spark.sql(f\"SELECT snapshot_id, parent_id, committed_at, operation, summary, manifest_list FROM {table_name}.snapshots ORDER BY committed_at DESC\")\n",
    "print(f\"Total de snapshots encontrados: {snapshots_df.count()} (esperado: 4+ ap√≥s DDL/INSERT/UPDATE/DELETE)\")\n",
    "print(\"Snapshots recentes (hist√≥rico completo - operation e summary chave):\")\n",
    "snapshots_df.show(truncate=False, n=10)  # Mostra colunas chave (√∫til para debug)\n",
    "\n",
    "if snapshots_df.count() < 2:\n",
    "    print(\"‚ö†Ô∏è Poucos snapshots - rode mais DMLs (INSERT/UPDATE/DELETE) para demo completa.\")\n",
    "else:\n",
    "    # Coleta snapshots para indexa√ß√£o (poucos, OK para .collect())\n",
    "    snapshots_list = snapshots_df.collect()\n",
    "    \n",
    "    # IDs chave (baseado em ordem DESC: [0]=mais recente (ap√≥s DELETE), [-1]=mais antigo (inicial))\n",
    "    current_snapshot_id = snapshots_list[0][0]  # snapshot_id da posi√ß√£o 0 (coluna 0)\n",
    "    v1_insert_id = snapshots_list[1][0] if len(snapshots_list) > 1 else None  # Ap√≥s INSERT\n",
    "    v2_update_id = snapshots_list[2][0] if len(snapshots_list) > 2 else None  # Ap√≥s UPDATE (antes DELETE)\n",
    "    old_snapshot_id = snapshots_list[-1][0]  # Mais antigo (inicial, DDL)\n",
    "    \n",
    "    # Fun√ß√£o auxiliar para query em snapshot espec√≠fico (foco em fict√≠cia + S√£o Paulo)\n",
    "    def query_snapshot(snapshot_id, limit=2):\n",
    "        return spark.sql(f\"\"\"\n",
    "            SELECT `C√≥digo IBGE`, `Munic√≠pio`, `UF`, `√çndice de Progresso Social` \n",
    "            FROM {table_name} VERSION AS OF {snapshot_id} \n",
    "            WHERE `C√≥digo IBGE` IN (9999999, 3550308)  -- Fict√≠cia + S√£o Paulo (ajuste 3550308 se outro)\n",
    "            ORDER BY `√çndice de Progresso Social` DESC\n",
    "        \"\"\")\n",
    "\n",
    "    # 1. Vers√£o Inicial (mais antigo: Ap√≥s DDL, antes INSERT/UPDATE/DELETE)\n",
    "    print(f\"\\n1. Vers√£o INICIAL (snapshot {old_snapshot_id} - ap√≥s DDL, dados originais):\")\n",
    "    try:\n",
    "        initial_df = query_snapshot(old_snapshot_id)\n",
    "        initial_df.show(truncate=False)\n",
    "        initial_count = spark.sql(f\"SELECT COUNT(*) FROM {table_name} VERSION AS OF {old_snapshot_id}\").collect()[0][0]\n",
    "        print(f\"   Linhas totais: {initial_count} (CSV original)\")\n",
    "        print(\"   Observa√ß√£o: Sem fict√≠cia (9999999); IPS original de S√£o Paulo (~66)\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Erro na query inicial: {e} (snapshot pode n√£o existir)\")\n",
    "\n",
    "    # 2. Ap√≥s INSERT (com fict√≠cia adicionada, antes UPDATE/DELETE)\n",
    "    if v1_insert_id:\n",
    "        print(f\"\\n2. Ap√≥s INSERT (snapshot {v1_insert_id} - fict√≠cia adicionada):\")\n",
    "        try:\n",
    "            insert_df = query_snapshot(v1_insert_id)\n",
    "            insert_df.show(truncate=False)\n",
    "            insert_count = spark.sql(f\"SELECT COUNT(*) FROM {table_name} VERSION AS OF {v1_insert_id}\").collect()[0][0]\n",
    "            print(f\"   Linhas totais: {insert_count} (+1 da fict√≠cia 9999999 com IPS 99.9 no topo)\")\n",
    "        except Exception as e:\n",
    "            print(f\"   Erro na query INSERT: {e}\")\n",
    "    else:\n",
    "        print(\"\\n2. Pulando ap√≥s INSERT (snapshot n√£o encontrado)\")\n",
    "\n",
    "    # 3. Ap√≥s UPDATE, Antes DELETE (fict√≠cia ainda presente, S√£o Paulo atualizado)\n",
    "    if v2_update_id:\n",
    "        print(f\"\\n3. Ap√≥s UPDATE, ANTES DELETE (snapshot {v2_update_id} - UPDATE aplicado):\")\n",
    "        try:\n",
    "            update_df = query_snapshot(v2_update_id)\n",
    "            update_df.show(truncate=False)\n",
    "            update_count = spark.sql(f\"SELECT COUNT(*) FROM {table_name} VERSION AS OF {v2_update_id}\").collect()[0][0]\n",
    "            print(f\"   Linhas totais: {update_count} (mesmo que ap√≥s INSERT; IPS de S√£o Paulo = 70.5)\")\n",
    "            print(\"   Observa√ß√£o: Fict√≠cia (9999999) ainda existe; S√£o Paulo atualizado\")\n",
    "        except Exception as e:\n",
    "            print(f\"   Erro na query UPDATE: {e}\")\n",
    "    else:\n",
    "        print(\"\\n3. Pulando ap√≥s UPDATE (snapshot n√£o encontrado)\")\n",
    "\n",
    "    # 4. Ap√≥s DELETE (atual: fict√≠cia removida, UPDATE preservado)\n",
    "    print(f\"\\n4. Ap√≥s DELETE (snapshot {current_snapshot_id} - atual, fict√≠cia removida):\")\n",
    "    try:\n",
    "        delete_df = query_snapshot(current_snapshot_id)\n",
    "        delete_df.show(truncate=False)\n",
    "        current_count = spark.sql(f'SELECT COUNT(*) FROM {table_name}').collect()[0][0]\n",
    "        print(f\"   Linhas totais: {current_count} (-1 da fict√≠cia; volta ao inicial)\")\n",
    "        print(\"   Observa√ß√£o: Fict√≠cia (9999999) sumiu; S√£o Paulo mant√©m IPS 70.5 (UPDATE intacto)\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Erro na query DELETE: {e}\")\n",
    "\n",
    "    # Resumo de Mudan√ßas (se snapshots suficientes)\n",
    "    if len(snapshots_list) >= 4:\n",
    "        print(f\"\\nResumo de Mudan√ßas:\")\n",
    "        print(f\"   Inicial (snapshot {old_snapshot_id}): {initial_count} linhas\")\n",
    "        print(f\"   Ap√≥s INSERT (snapshot {v1_insert_id}): {insert_count} linhas (+1)\")\n",
    "        print(f\"   Ap√≥s UPDATE (snapshot {v2_update_id}): {update_count} linhas (sem mudan√ßa)\")\n",
    "        print(f\"   Ap√≥s DELETE (snapshot {current_snapshot_id}): {current_count} linhas (-1)\")\n",
    "\n",
    "# Opcional: Rollback para snapshot antes do DELETE (ex: v2 - desfaz DELETE, fict√≠cia volta - CUIDADO!)\n",
    "# if v2_update_id:\n",
    "#     spark.sql(f\"CALL iceberg_system.rollback_to_snapshot('{table_name}', {v2_update_id})\")\n",
    "#     print(f\"Rollback executado para snapshot {v2_update_id}! Fict√≠cia voltou - COUNT(*): {spark.sql(f'SELECT COUNT(*) FROM {table_name}').collect()[0][0]}\")\n",
    "#     # Re-execute DELETE para fixar se quiser\n",
    "\n",
    "print(\"\\nTime Travel Iceberg completo! Hist√≥rico imut√°vel - INSERT/UPDATE/DELETE audit√°veis em snapshots espec√≠ficos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e8de1e-45a6-4ab1-b64e-0b36f8f7af3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
